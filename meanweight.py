# -*- coding: utf-8 -*-
"""MeanWeight.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LnCmVOMK_Djp3ClJ6AmoagEyQE6bVd-N
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.impute import SimpleImputer
from sklearn.cluster import MeanShift

# Load the dataset
data = pd.read_csv('/content/STOCKS_Dataset.csv')
# Inspect the data
# print(data.head())
# Load the dataset
# print("Original data shape:", data.shape)
# print("Data columns:", data.columns)
# print(data.head())

# Extract company names
companies = data.columns[1:]
# print(companies)
# Drop the date column and transpose the data for clustering
data = data.drop(columns=['Date']).T
data = data.replace(',', '', regex=True).astype(float)

# Inspect the transposed data
# print(data.head())

# Assuming weights are equally distributed initially
# weights = np.ones(data.shape[1])

# Normalize the data if necessary
# data = (data - data.mean(axis=1, keepdims=True)) / data.std(axis=1, keepdims=True)
# Normalize the data if necessary
# Ensure data is a numpy array
# print(data[0].shape)
data = data.values
imputer = SimpleImputer(strategy='mean')
data = imputer.fit_transform(data)
print(data.shape)

if data.ndim == 1:
    # If data is 1D, calculate mean and std without axis
    data = (data - data.mean()) / data.std()
else:
    # If data is 2D or more, calculate mean and std along axis 1
    data = (data - data.mean(axis=1, keepdims=True)) / data.std(axis=1, keepdims=True)
print(data)

# class WeightedAdaptiveMeanShift:
#     def __init__(self, bandwidth, max_iter=300, tol=1e-3):
#         self.bandwidth = bandwidth
#         self.max_iter = max_iter
#         self.tol = tol

#     def fit(self, X, weights):
#         n_points, n_features = X.shape
#         centroids = np.copy(X)

#         for _ in range(self.max_iter):
#             for i in range(n_points):
#                 # Calculate the weighted mean of points within the current bandwidth
#                 distances = np.linalg.norm(X - centroids[i], axis=1)
#                 within_bandwidth = distances < self.bandwidth
#                 if np.sum(within_bandwidth) == 0:
#                     continue
#                 weighted_mean = np.average(X[within_bandwidth], axis=0, weights=weights[within_bandwidth])

#                 # Shift point towards the weighted mean
#                 shift = np.linalg.norm(weighted_mean - centroids[i])
#                 centroids[i] = weighted_mean

#                 if np.all(np.linalg.norm(centroids - X, axis=1) < self.tol):
#                     break

#         # Remove duplicates and assign points to nearest centroid
#         unique_centroids = np.unique(centroids, axis=0)
#         labels = np.argmin(np.linalg.norm(X[:, np.newaxis] - unique_centroids, axis=2), axis=1)

#         return unique_centroids, labels



# data = data.T

# Debugging: Print lengths
# print(f"Number of companies: {len(companies)}")
# print(f"Number of data points (transposed rows): {data.shape[0]}")

# Ensure that the number of companies matches the transposed data
# assert len(companies) == data.shape[0], "Mismatch in number of companies and transposed data rows."

# Instantiate MeanShift with a given bandwidth
bandwidth = 4  # You can adjust this based on your data
mean_shift = MeanShift(bandwidth=bandwidth)

# Fit MeanShift to the normalized data
# data = np.reshape(data.T,(50,50))
mean_shift.fit(data.T)
print(data.shape)
print(data)


# Extract centroids and labels
centroids = mean_shift.cluster_centers_
labels = mean_shift.labels_

# print("Centroids:", centroids)
# print("Labels:", labels)

cluster_labels = np.unique(labels)
n_cluster = len(cluster_labels)
print(n_cluster)

# Debugging: Print lengths of companies and labels
print(f"Number of labels: {len(labels)}")

# # Create a DataFrame to map companies to clusters
# company_cluster_df = pd.DataFrame({'Company': companies, 'Cluster': labels})

# # Display the DataFrame showing each company and its cluster
# print(company_cluster_df)

# # Calculate the frequency of companies in each cluster
# cluster_counts = company_cluster_df['Cluster'].value_counts().reset_index()
# cluster_counts.columns = ['Cluster', 'Frequency']

# # Merge the two DataFrames to show companies and their cluster frequencies
# company_cluster_freq_df = company_cluster_df.merge(cluster_counts, on='Cluster')

# # Print the resulting DataFrame
# print(company_cluster_freq_df)

# # Example usage on the stock dataset
data_values = data.T


# # Plot the clusters
plt.figure(figsize=(10, 6))
for i in range(len(centroids)):
    plt.scatter(data_values[labels == i, 0], data_values[labels == i, 1], label=f'Cluster {i+1}')

plt.scatter(centroids[:, 0], centroids[:, 1], s=300, c='red', label='Centroids')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.legend()
plt.title('Weighted Adaptive Mean Shift Clustering')
plt.show()

# if len(companies) == len(labels):
    # Create a DataFrame to map companies to clusters
    # company_cluster_df = pd.DataFrame({'Company': companies, 'Cluster': labels})

    # Display the DataFrame showing each company and its cluster
    # print(company_cluster_df)

    # Calculate the frequency of companies in each cluster
    # cluster_counts = company_cluster_df['Cluster'].value_counts().reset_index()
    # cluster_counts.columns = ['Cluster', 'Frequency']

    # Merge the two DataFrames to show companies and their cluster frequencies
    # company_cluster_freq_df = company_cluster_df.merge(cluster_counts, on='Cluster')

    # Print the resulting DataFrame
    # print(company_cluster_freq_df)

    # Plot the clusters
    # plt.figure(figsize=(10, 6))
    # for i in range(len(centroids)):
    #     plt.scatter(data[labels == i, 0], data[labels == i, 1], label=f'Cluster {i+1}')

    # plt.scatter(centroids[:, 0], centroids[:, 1], s=300, c='red', label='Centroids')
    # plt.xlabel('Feature 1')
    # plt.ylabel('Feature 2')
    # plt.legend()
    # plt.title('Weighted Adaptive Mean Shift Clustering')
    # plt.show()
# else:
#     print("Mismatch in number of companies and labels. Please check the data preprocessing steps.")